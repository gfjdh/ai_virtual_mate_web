import io
from vlm import *

with open('data/db/memory.db', 'r', encoding='utf-8') as memory_file:
    try:
        openai_history = json.load(memory_file)
    except:
        openai_history = []


def chat_preprocess(msg):  # èŠå¤©é¢„å¤„ç†
    try:
        if ("å±å¹•" in msg or "ç”»é¢" in msg or "å›¾åƒ" in msg or "çœ‹åˆ°" in msg or "çœ‹çœ‹" in msg or "çœ‹è§" in msg
              or "ç…§ç‰‡" in msg or "æ‘„åƒå¤´" in msg or "å›¾ç‰‡" in msg) and img_menu.get() != "å…³é—­å›¾åƒè¯†åˆ«":
            if mode_menu.get() == "è§’è‰²æ‰®æ¼”èŠå¤©":
                msg = f"{prompt}ã€‚ä½ éœ€è¦æ ¹æ®å…¶ä¸­å†…å®¹å’Œæˆ‘èŠå¤©ã€‚æˆ‘çš„é—®é¢˜æ˜¯ï¼š{msg}"
            vlm_mapping = {
                "photo": {"GLM-4V-Flash": glm_4v_photo, "æœ¬åœ°Ollama VLM": ollama_vlm_photo,
                          "æœ¬åœ°LM Studio": lmstudio_vlm_photo, "æœ¬åœ°QwenVLæ•´åˆåŒ…": qwen_vlm_photo,
                          "æœ¬åœ°Janusæ•´åˆåŒ…": janus_photo, "è‡ªå®šä¹‰API-VLM": custom_vlm_photo},
                "screen": {"GLM-4V-Flash": glm_4v_screen, "æœ¬åœ°Ollama VLM": ollama_vlm_screen,
                           "æœ¬åœ°LM Studio": lmstudio_vlm_screen, "æœ¬åœ°QwenVLæ•´åˆåŒ…": qwen_vlm_screen,
                           "æœ¬åœ°Janusæ•´åˆåŒ…": janus_screen, "è‡ªå®šä¹‰API-VLM": custom_vlm_screen},
                "cam": {"GLM-4V-Flash": glm_4v_cam, "æœ¬åœ°Ollama VLM": ollama_vlm_cam, "æœ¬åœ°LM Studio": lmstudio_vlm_cam,
                        "æœ¬åœ°QwenVLæ•´åˆåŒ…": qwen_vlm_cam, "æœ¬åœ°Janusæ•´åˆåŒ…": janus_cam, "è‡ªå®šä¹‰API-VLM": custom_vlm_cam}}
            if "å›¾ç‰‡" in msg:
                if os.path.exists("data/cache/cache.png"):
                    selected_model = img_menu.get()
                    content = vlm_mapping["photo"][selected_model](msg)
                    notice(f"{mate_name}è¯†åˆ«äº†ä¸Šä¼ çš„å›¾ç‰‡")
                    os.remove("data/cache/cache.png")
                else:
                    content = "è¯·å…ˆç‚¹å‡»å³ä¸‹æ–¹æŒ‰é’®ä¸Šä¼ å›¾ç‰‡"
                    notice("è¯·å…ˆç‚¹å‡»å³ä¸‹æ–¹æŒ‰é’®ä¸Šä¼ å›¾ç‰‡")
            elif any(keyword in msg for keyword in ["çœ‹åˆ°", "çœ‹è§", "çœ‹çœ‹", "ç…§ç‰‡", "æ‘„åƒå¤´"]) and cam_permission == "å¼€å¯":
                selected_model = img_menu.get()
                content = vlm_mapping["cam"][selected_model](msg)
                notice(f"{mate_name}æ‹äº†ç…§ç‰‡ï¼Œè°ƒç”¨[æ‘„åƒå¤´è¯†åˆ«]")
            else:
                selected_model = img_menu.get()
                content = vlm_mapping["screen"][selected_model](msg)
                notice(f"{mate_name}æ•è·äº†å±å¹•ï¼Œè°ƒç”¨[ç”µè„‘å±å¹•è¯†åˆ«]")
        elif "ç”»" in msg and prefer_draw != "å…³é—­AIç»˜ç”»":
            msg = re.sub(r"ç”»|ç»˜ç”»", "", msg)
            content = "æ­£åœ¨è¿›è¡ŒAIç»˜ç”»"
            notice(f"{mate_name}æ­£åœ¨è¿›è¡ŒAIç»˜ç”»ï¼Œè¯·ç¨ç­‰...")
            if prefer_draw == "æœ¬åœ°SD API":
                local_sd(msg)
            elif prefer_draw == "æœ¬åœ°Janusæ•´åˆåŒ…":
                local_janus(msg)
            elif prefer_draw == "äº‘ç«¯CogView-3":
                cloud_cogview(msg)
            elif prefer_draw == "äº‘ç«¯Kolors":
                cloud_kolors(msg)
            elif prefer_draw == "äº‘ç«¯æ–‡å¿ƒWeb":
                content = "ç»˜ç”»å®Œæˆ"
                msg = re.sub(r"ç”»|ç»˜ç”»", "", msg)
                wb.open(f'https://image.baidu.com/front/aigc?tn=aigc&word={msg}')
                notice(f"{mate_name}æ‰“å¼€äº†æµè§ˆå™¨ï¼Œè°ƒç”¨[äº‘AIç»˜ç”»]")
        else:
            if mode_menu.get() == "å¤šæ™ºèƒ½ä½“åŠ©æ‰‹":
                content = function_llm(f"{prompt}ã€‚ä½ æ˜¯{mate_name}ï¼Œæ˜¯ä¸“å±äºæˆ‘({username})çš„å¤šæ™ºèƒ½ä½“åŠ©æ‰‹ï¼Œæ”¯æŒè°ƒç”¨å¤šç§æ™ºèƒ½ä½“ï¼Œæ‹¥æœ‰ä»¥ä¸‹åŠŸèƒ½ï¼š{all_task}ã€‚", msg)
            else:
                content = chat_llm(msg)
            notice(f"æ”¶åˆ°{mate_name}å›å¤")
        with open('data/db/memory.db', 'w', encoding='utf-8') as f:
            json.dump(openai_history, f, ensure_ascii=False, indent=4)
        return content
    except Exception as e:
        notice(f"å›¾åƒè¯†åˆ«å¼•æ“é…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}")
        return "å›¾åƒè¯†åˆ«å¼•æ“é…ç½®é”™è¯¯"


def chat_llm(msg):  # å¤§è¯­è¨€æ¨¡å‹èŠå¤©
    prompt1 = prompt + "/no_think"
    if "å‡ ç‚¹" in msg or "å¤šå°‘ç‚¹" in msg or "æ—¶é—´" in msg or "æ—¶å€™" in msg or "æ—¥æœŸ" in msg or "å¤šå°‘å·" in msg or "å‡ å·" in msg:
        msg = f"[å½“å‰æ—¶é—´:{current_time()}]{msg}"
    try:
        if llm_menu.get() == "GLM-4-Flash":
            client = ZhipuAI(api_key=glm_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": prompt1}]
            messages.extend(openai_history)
            completion = client.chat.completions.create(model=glm_llm_model, messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content.strip()
        elif llm_menu.get() == "DeepSeek-R1-8B":
            client = OpenAI(base_url=sf_url, api_key=sf_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": prompt1}]
            messages.extend(openai_history)
            completion = client.chat.completions.create(model="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                                                        messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            res = completion.choices[0].message.content
            if think_filter_switch == "on":
                res = res.split("</think>")[-1].strip()
            return res
        elif llm_menu.get() == "é€šä¹‰åƒé—®3-8B":
            client = OpenAI(base_url=sf_url, api_key=sf_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": prompt1}]
            messages.extend(openai_history)
            completion = client.chat.completions.create(model="Qwen/Qwen3-8B", messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content.strip()
        elif llm_menu.get() == "æ–‡å¿ƒä¸€è¨€Speed":
            client = OpenAI(base_url=bd_url, api_key=bd_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": prompt1}]
            messages.extend(openai_history)
            completion = client.chat.completions.create(model=bd_model, messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "è…¾è®¯æ··å…ƒLite":
            client = OpenAI(base_url=hy_url, api_key=hy_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": prompt1}]
            messages.extend(openai_history)
            completion = client.chat.completions.create(model=hy_model, messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "è®¯é£æ˜Ÿç«Lite":
            client = OpenAI(base_url=xf_url, api_key=xf_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": prompt1}]
            messages.extend(openai_history)
            completion = client.chat.completions.create(model=xf_model, messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "æœ¬åœ°Transformers":
            try:
                client = OpenAI(base_url=f"http://{local_llm_ip}:{tf_port}/v1", api_key="transformers")
                openai_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": prompt1}]
                messages.extend(openai_history)
                completion = client.chat.completions.create(model=tf_model, messages=messages, stream=False)
                chunks = re.findall(r'^data:\s*(.+)$', completion, flags=re.MULTILINE)
                result_content = []
                for chunk in chunks:
                    try:
                        data = json.loads(chunk)
                        content = data.get("choices", [{}])[0].get("delta", {}).get("content")
                        if content is not None:
                            result_content.append(content)
                    except json.JSONDecodeError:
                        continue
                result_content = "".join(result_content).replace("\n", "")
                openai_history.append({"role": "assistant", "content": result_content})
                res = result_content
                if think_filter_switch == "on":
                    res = res.split("</think>")[-1].strip()
                return res
            except Exception as e:
                return f"æœ¬åœ°TransformersæœåŠ¡æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "æœ¬åœ°LM Studio":
            try:
                client = OpenAI(base_url=f"http://{local_llm_ip}:{lmstudio_port}/v1", api_key="lm-studio")
                openai_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": prompt1}]
                messages.extend(openai_history)
                completion = client.chat.completions.create(model="", messages=messages)
                openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
                res = completion.choices[0].message.content
                if think_filter_switch == "on":
                    res = res.split("</think>")[-1].strip()
                return res
            except Exception as e:
                return f"æœ¬åœ°LM Studioè½¯ä»¶APIæœåŠ¡æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "æœ¬åœ°Ollama LLM":
            try:
                try:
                    rq.get(f'http://{local_llm_ip}:{ollama_port}')
                except:
                    Popen(f"ollama pull {ollama_model_name}", shell=False)
                client = Client(host=f'http://{local_llm_ip}:{ollama_port}')
                openai_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": prompt1}]
                messages.extend(openai_history)
                response = client.chat(model=ollama_model_name, messages=messages)
                openai_history.append({"role": "assistant", "content": response['message']['content']})
                res = response['message']['content']
                if think_filter_switch == "on":
                    res = res.split("</think>")[-1].strip()
                return res
            except Exception as e:
                return f"æœ¬åœ°Ollama LLMé…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "DifyèŠå¤©åŠ©æ‰‹":
            try:
                res = chat_dify(msg)
                return res
            except Exception as e:
                return f"æœ¬åœ°DifyèŠå¤©åŠ©æ‰‹é…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "AnythingLLM":
            try:
                res = chat_anything_llm(msg)
                return res
            except Exception as e:
                return f"æœ¬åœ°AnythingLLMçŸ¥è¯†åº“é…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        else:
            try:
                client = OpenAI(base_url=custom_url, api_key=custom_key)
                openai_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": prompt1}]
                messages.extend(openai_history)
                completion = client.chat.completions.create(model=custom_model, messages=messages)
                openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
                res = completion.choices[0].message.content
                if think_filter_switch == "on":
                    res = res.split("</think>")[-1].strip()
                return res.strip()
            except Exception as e:
                return f"è‡ªå®šä¹‰APIé…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
    except:
        return f"{llm_menu.get()}æœåŠ¡æœªæ­£ç¡®è®¾ç½®ï¼Œè¯·å‰å¾€è½¯ä»¶è®¾ç½®â†’äº‘ç«¯AI Keyè®¾ç½®"


# open_source_project_address:https://github.com/swordswind/ai_virtual_mate_web
def chat_dify(msg):  # DifyèŠå¤©åŠ©æ‰‹
    headers = {"Authorization": f"Bearer {dify_key}", "Content-Type": "application/json"}
    data = {"query": msg, "inputs": {}, "response_mode": "blocking", "user": username, "conversation_id": None}
    res = rq.post(f"http://{dify_ip}/v1/chat-messages", headers=headers, data=json.dumps(data))
    res = res.json()['answer'].strip()
    if think_filter_switch == "on":
        res = res.split("</think>")[-1].strip()
    return res


def chat_anything_llm(msg):  # AnythingLLMçŸ¥è¯†åº“
    url = f"http://{local_llm_ip}:3001/api/v1/workspace/{anything_llm_ws}/chat"
    headers = {"Authorization": f"Bearer {anything_llm_key}", "Content-Type": "application/json"}
    data = {"message": msg}
    res = rq.post(url, json=data, headers=headers)
    res = res.json().get("textResponse")
    if think_filter_switch == "on":
        res = res.split("</think>")[-1].strip()
    return res


def clear_chat():  # åˆ é™¤èŠå¤©è®°å½•
    global openai_history
    if messagebox.askokcancel(f"æ¸…é™¤{mate_name}çš„èŠå¤©è®°å½•",
                              f"æ‚¨ç¡®å®šè¦æ¸…é™¤{mate_name}çš„èŠå¤©è®°å½•å—ï¼Ÿ\nå¦‚æœ‰éœ€è¦å¯å…ˆç‚¹å‡»ğŸ”¼å¯¼å‡ºè®°å½•å†å¼€å¯æ–°å¯¹è¯\n(è¯¥æ“ä½œä¸å½±å“ä¼™ä¼´è®°å¿†ï¼Œ\nå¦‚æœæƒ³åˆ é™¤è®°å¿†å¯å³é”®èŠå¤©æ¡†)"):
        output_box.delete("1.0", "end")
        notice("èŠå¤©è®°å½•å·²æ¸…ç©º")


def local_sd(msg):  # æœ¬åœ°Stable Diffusion
    def local_sd_th():
        try:
            sd_prompt = function_llm("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šä¸”é«˜æ°´å¹³çš„Stable Diffusion AIç»˜ç”»æç¤ºè¯ç”Ÿæˆå™¨ï¼Œæ“…é•¿æŠŠæˆ‘çš„è‡ªç„¶è¯­è¨€è½¬æ¢æˆStable Diffusion AIç»˜ç”»è‹±æ–‡æç¤ºè¯ã€‚å›ç­”åªéœ€è¾“å‡ºAIç»˜ç”»è‹±æ–‡æç¤ºè¯ï¼Œæç¤ºè¯ç”±è‹±æ–‡å•è¯ç»„æˆï¼Œç”¨è‹±æ–‡é€—å·éš”å¼€ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚",
                                     f"è¯·ä½ æ ¹æ®æˆ‘çš„ä¸‹è¿°éœ€æ±‚ç”ŸæˆAIç»˜ç”»æç¤ºè¯ï¼Œæç¤ºè¯ç”±è‹±æ–‡å•è¯ç»„æˆï¼Œç”¨è‹±æ–‡é€—å·éš”å¼€ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼š{msg}")
            payload = {"prompt": sd_prompt, "steps": 20}
            res = rq.post(f"http://{local_draw_ip}:{sd_port}/sdapi/v1/txt2img", json=payload)
            data = res.json()
            sd_draw_path = "data/cache/draw/sd_aigc.png"
            with open(sd_draw_path, 'wb') as f:
                f.write(b64decode(data['images'][0]))
            notice("ç»˜ç”»å®Œæˆ")
            draw_box("Stable Diffusion", "sd_aigc")
        except Exception as e:
            notice(f"æœ¬åœ°SD AIç»˜ç”»å‡ºé”™ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}")

    Thread(target=local_sd_th).start()


def local_janus(msg):  # æœ¬åœ°Janus
    def local_janus_th():
        try:
            janus_prompt = function_llm("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šä¸”é«˜æ°´å¹³çš„ç¿»è¯‘å®˜ï¼Œæ“…é•¿æŠŠæˆ‘çš„è¯­è¨€ç¿»è¯‘æˆè‹±æ–‡ã€‚å›ç­”åªéœ€è¾“å‡ºç¿»è¯‘æˆè‹±æ–‡çš„ç»“æœï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚",
                                        f"æŠŠä¸‹åˆ—å†…å®¹ç¿»è¯‘æˆè‹±æ–‡ï¼Œå›ç­”åªéœ€è¾“å‡ºç¿»è¯‘æˆè‹±æ–‡çš„ç»“æœï¼š{msg}")
            data = {'prompt': janus_prompt, 'seed': None, 'guidance': 5}
            response = rq.post(f"http://{local_draw_ip}:8082/generate_images/", data=data, stream=True)
            buffer = io.BytesIO()
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:
                    buffer.write(chunk)
            buffer.seek(0)
            image = Image.open(buffer)
            janus_draw_path = "data/cache/draw/janus_aigc.png"
            image.save(janus_draw_path)
            notice("ç»˜ç”»å®Œæˆ")
            draw_box("Janus", "janus_aigc")
        except Exception as e:
            notice(f"æœ¬åœ°Janus AIç»˜ç”»å‡ºé”™ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}")

    Thread(target=local_janus_th).start()


def cloud_cogview(msg):  # äº‘ç«¯CogView
    def cloud_cogview_th():
        try:
            client = ZhipuAI(api_key=glm_key)
            res = client.images.generations(model="cogview-3-flash", prompt=msg)
            image_response = rq.get(res.data[0].url)
            cogview_draw_path = "data/cache/draw/cogview_aigc.png"
            with open(cogview_draw_path, "wb") as f:
                f.write(image_response.content)
            notice("ç»˜ç”»å®Œæˆ")
            draw_box("CogView", "cogview_aigc")
        except:
            notice("äº‘ç«¯Cogviewç»˜ç”»æœªæ­£ç¡®é…ç½®ï¼Œè¯·å‰å¾€è½¯ä»¶è®¾ç½®â†’äº‘ç«¯AI Keyè®¾ç½®GLMæ™ºè°±BigModelå¼€æ”¾å¹³å°key")

    Thread(target=cloud_cogview_th).start()


def cloud_kolors(msg):  # äº‘ç«¯Kolors
    def cloud_kolors_th():
        try:
            url = f"{sf_url}/images/generations"
            payload = {"model": "Kwai-Kolors/Kolors", "prompt": msg}
            headers = {"Authorization": f"Bearer {sf_key}", "Content-Type": "application/json"}
            res = rq.request("POST", url, json=payload, headers=headers)
            image_response = rq.get(res.json()["images"][0]["url"])
            kolors_draw_path = "data/cache/draw/kolors_aigc.png"
            with open(kolors_draw_path, "wb") as f:
                f.write(image_response.content)
            notice("ç»˜ç”»å®Œæˆ")
            draw_box("Kolors", "kolors_aigc")
        except:
            notice("äº‘ç«¯Kolorsç»˜ç”»æœªæ­£ç¡®é…ç½®ï¼Œè¯·å‰å¾€è½¯ä»¶è®¾ç½®â†’äº‘ç«¯AI Keyè®¾ç½®SiliconCloudç¡…åŸºæµåŠ¨å¹³å°key")

    Thread(target=cloud_kolors_th).start()
